{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwALoygGD617"
   },
   "source": [
    "# **HOMEWORK 1 (110 points + 1) : CAUTION! CONTENTS ARE HOT** üåã\n",
    "## **DUE: *SEPTEMBER 18, 2025 @ 11:59 PM***\n",
    "## **24-HR LATE DUE DATE WITH A 15% PENALTY: *SEPTEMBER 19, 2025 @ 11:59 PM***\n",
    "\n",
    "The [NCEI/WDS Global Significant Volcanic Eruptions Database](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.mgg.hazards:G10147) is a very comprehensive collection of +600 volcanic eruptions dating from 4360 BC to the present. Due to the nature of this assignment, we will be dealing with relatively newer volcanoes (in which some are still obviously still older than anyone on Earth currently). Each eruption in the database is classified as significant if it meets one or more criteria, such as causing fatalities, incurring **damage on property** (**+$1 million**), reaching a **Volcanic Explosivity Index (VEI)** of **6 or higher**, generating a tsunami, or being linked to a significant earthquake. The VEI is a scale that measures the explosiveness of volcanic eruptions, providing insight into the magnitude and potential consequences of the eruptions. The database includes detailed information on the location, type of volcano, last known eruption, VEI, casualties, property damage, and much more.\n",
    "![volcano](https://wikitravel.org/upload/shared//9/99/Volcano_de_Fuego_Banner.jpg)\n",
    "**We are going to dive straight into these volcanoes (well... their dataset), to swim our way into Pandas proficiency!**\n",
    "\n",
    "You will find the [Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html) helpful. There are also some helpful links to guide you along the way! Don't get burned üî•\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sQEC_lpHYdq"
   },
   "source": [
    "# **For this assignment, we are including a special bonus question to help everyone get familiar with the proper assignment submission routine. We strongly encourage you to complete Bonus Question 1, learn the process, and follow this practice for all future assignments.**\n",
    "\n",
    "# **For this assignment only, you will not lose points if you skip the bonus question. However, starting from Homework 2, submitting both the Jupyter Notebook file and the PDF will be required in order to receive full credit.**\n",
    "\n",
    "### **DO NOT REMOVE ANY PART OF ANY OF THE QUESTIONS OR YOU LOSE CREDIT**\n",
    "### *No Hardcoding either*  üòã‚ù§Ô∏è‚Äçüî•\n",
    "### **REMEMBER TO SHOW OUTPUT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrD3qZmJUp6C"
   },
   "source": [
    "### **Part 1: Maintenance ü§© (25 POINTS TOTAL)**\n",
    "First, we're going to familiarize ourselves with the process. As in most languages, Python looks best when its modules are imported first before any other code is written ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CruzvUOrKEoZ"
   },
   "outputs": [],
   "source": [
    "# Make sure these code blocks run properly and that you have properly installed the appropriate modules required.\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy\n",
    "import math\n",
    "# import other libraries here\n",
    "\n",
    "# Don't Remove this\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nWwbZ4jK5Wi"
   },
   "source": [
    "As you may have noticed, there's another library aside from Pandas called \"[requests](https://requests.readthedocs.io/en/latest/).\" **The requests library allows you to send HTTP requests to a server, retrieve the content, and process it at ease.** It's very beginner friendly for those attempting to get into webscraping (super important for collecting and creating datasets). We also recommend looking into [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) (yeah, soup LOL), another wonderful library that can be paired with the requests library for webscraping.\n",
    "\n",
    "As shown below, sometimes specific websites require specific headers in order to process a request to access the data.\n",
    "\n",
    "To check if a request was processed successfully, use the [status_code](https://requests.readthedocs.io/en/latest/api/) function to see if the process returned 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMxyFjVrKKvG"
   },
   "outputs": [],
   "source": [
    "# API URL and headers in case request gets denied.\n",
    "api_url = \"https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/volcanoes\"\n",
    "\n",
    "headers = {\n",
    "    'accept': '*/*'\n",
    "}\n",
    "\n",
    "data = requests.get(api_url)\n",
    "data.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwJMQjBmJ5Ly"
   },
   "source": [
    "#### **TASK 1.0: Cute Webscraping (5 points)**\n",
    "To make our cute webscraper we need to **create a GET request** using the relevant information given above.\n",
    "\n",
    "This particular dataset NOAA returns data from the API as ***json*** when a user makes a request. The json data has a particular format, so we will extract our needed information only from the field called **items** to make a dataframe.\n",
    "\n",
    "**After properly scraping the data, name this dataframe** ***df***\n",
    "\n",
    "**Save this dataframe into a CSV file named 'volcanoes.csv'**\n",
    "\n",
    "**You won't need to run this more than once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pRfxOpACC39n"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.json()[\"items\"])\n",
    "df.to_csv(\"volcanoes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geMtNQeiEo8U"
   },
   "source": [
    "#### **TASK 1.1: 1 Liner Thingz (3 points)**\n",
    "\n",
    "We need to get an idea of what this dataset is going to look. In order to do that, let's take a look at some of the most [basic things](https://dataanalytics.buffalostate.edu/pandas-cheat-sheet) our dataframe has.\n",
    "\n",
    "**Read the directions carefully and code your answer with only one line of code.**\n",
    "\n",
    "***CAN'T USE LOOPS. DO NOT DISPLAY THE DATAFRAME, JUST YOUR CODE OUTPUT HERE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Uyhc9VOJdbn"
   },
   "source": [
    "**1.1.1:** In one line of code and **using only one single method call**, output the **summary statistics** (count, mean, std, min, max, quartiles) of all numerical features in the dataframe.\n",
    "\n",
    "*Hint: Look for a method that gives a descriptive overview of numerical columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_PPlwLyJbbL"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoMluxmiESrd"
   },
   "source": [
    "**1.1.2:** In one line of code and **using only one single method call**, print the **summary information** of the dataframe, including index dtype, column names, non-null counts, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4XwpjUUETbU"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uvL0VpKNeAI"
   },
   "source": [
    "We won't be using some of the data because there is a lot of missing data.\n",
    "\n",
    "**1.1.3:** *In one line of code, create a **new dataframe** called **new_df** that **discards** all the features of the **old** dataframe **except for the following**:*\n",
    "\n",
    "`id,\tyear, month, day,\ttsunamiEventId, earthquakeEventId, volcanoLocationId, volcanoLocationNewNum, name, country, elevation, morphology, deathsTotal, vei, deaths`\n",
    "\n",
    "*Hint: Don't use any drop function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmFy8xLFLRrm"
   },
   "outputs": [],
   "source": [
    "new_df = df[[\"id\", \"year\", \"month\", \"day\",\t\"tsunamiEventId\", \"earthquakeEventId\", \"volcanoLocationId\", \"volcanoLocationNewNum\", \"name\", \"country\", \"elevation\", \"morphology\", \"deathsTotal\", \"vei\", \"deaths\"]].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8xP6O37UQp7"
   },
   "source": [
    "#### **TASK 1.2: 1 Liner Shenaniganz (7 points)**\n",
    "\n",
    "We're going to tidy up the **new dataframe** a little more with some more advanced 1 liner code.\n",
    "\n",
    "**Read the directions carefully and code your answer with only one line of code.**\n",
    "\n",
    "**For this section, keep the method of display that is already in the box. Write your code as indicated.**\n",
    "\n",
    "***YOU CAN'T USE ONE LINE LOOPS OR ANY KIND OF LOOP.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_SHOaZRIblA"
   },
   "source": [
    "**1.2.1:** *In one line of code, **drop any row** that contains **NaN** in **any one** of the columns indicating a measure of **time**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9qVE-OdGysV"
   },
   "outputs": [],
   "source": [
    "new_df.dropna(subset=[\"year\", \"day\", \"month\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO-A8f3gGEpn"
   },
   "source": [
    "**1.2.2:** *In one line of code, **change** the **index column** of the dataframe so that it has **1-based indexing**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBBUKUJVFI0Y"
   },
   "outputs": [],
   "source": [
    "new_df.index = numpy.arange(1, len(new_df) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSxQRt-xio1t"
   },
   "source": [
    "The **deathsTotal** and **deaths**  columns have approximations of the same data with alternating NaNs in each.\n",
    "\n",
    "**1.2.3:** *In one line of code, make a **new column** called **'totalDeaths'** that takes the **max** of the values given between those* ***two*** *columns.\n",
    "- If there is **NaN** in ***one column*** and a **numerical** value in the **other**, it will ***take the numerical value***.\n",
    "- **Only** if there are **NaNs** in ***both columns***, the **new column will have NaN.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEZ0YdSPdYyI"
   },
   "outputs": [],
   "source": [
    "new_df[\"totalDeaths\"] = new_df[[\"deaths\", \"deathsTotal\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9if9utm8jsdg"
   },
   "source": [
    "#### **TASK 1.3: Tailoring Time (10 Points)**\n",
    "It's pretty obvious that the year, month, and day look pretty weird in the dataset. We're going to have to do some hardcore cleaning on the [time](https://pandas.pydata.org/docs/user_guide/timeseries.html).\n",
    "\n",
    "**We need to have only ONE column called** \"***date***\" **that contains the full date (YYYY-MM-DD), not separated into three columns.**\n",
    "\n",
    "***Make sure there are no floating point values in the date and sort the data from most recent to least.***\n",
    "\n",
    "***Remove the old columns and place the new column next to the 'id' column.***\n",
    "\n",
    "\n",
    "**YOU CAN USE MULTIPLE LINES OF CODE BUT CAN'T USE LOOPS.**\n",
    "\n",
    "**Note:** It is alright to have only a **maximum of 12 NaTs** for some dates that often go further back than the 1600s because the datetime module in Pandas has a limit (unless otherwise guided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gs4GJqzgn2Es"
   },
   "outputs": [],
   "source": [
    "new_df[\"year\"] = new_df[\"year\"].apply(lambda x: None if x < 1677 else x)\n",
    "new_df.insert(loc=1, column=\"date\", value=pd.to_datetime(arg=new_df[[\"year\", \"month\", \"day\"]]))\n",
    "new_df.drop([\"year\", \"month\", \"day\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlhYym73iOZb"
   },
   "source": [
    "### **Part 2: Volcanic Matryoshkas ü™Ü (30 POINTS TOTAL)**\n",
    "\n",
    "Now, that most of the data has been tidied up. We will organize the data into more sizable pieces of information in order to extract useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaprrfcLDDQ_"
   },
   "source": [
    "**2.1.1:** *(10 points here)*\n",
    "\n",
    "Use the **groupby function in Pandas** to create separate dataframes for each unique country.\n",
    "\n",
    "* **Each table must only have the columns**: 'date' 'country', 'name', and 'vei' (Having the index is ok. Up to you.)\n",
    "\n",
    "* **Sort** the dataframe of **each country** by **highest to lowest 'vei'**\n",
    "\n",
    "* Use the **[display](https://ipython.readthedocs.io/en/8.26.0/api/generated/IPython.display.html)** function to show **each sorted table**\n",
    "\n",
    "**You MUST use the groupby function here and display your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YonGiaib0noL"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "separated_dfs = {}\n",
    "\n",
    "for group_name, group_df in new_df[[\"date\", \"country\", \"name\", \"vei\"]].groupby([\"country\"]):\n",
    "    group_df.sort_values(by=\"vei\", inplace=True, ascending=False)\n",
    "    separated_dfs[group_name] = group_df\n",
    "    display(group_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTExPGKLA7lV"
   },
   "source": [
    "**2.1.2:** *(5 points here)*\n",
    "\n",
    "Using **groupby again**, print out the **maximum 'vei'** for **each unique country.**\n",
    "\n",
    "**You MUST use the groupby function here and print your results.**\n",
    "\n",
    "* **Print** out your results in a format like the following: \"Country: {country_name}, Highest VEI: {vei}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRHCvg1b2MEA"
   },
   "outputs": [],
   "source": [
    "for country, fmax in new_df.groupby(\"country\")[\"vei\"].max().items():\n",
    "    print(f\"Country: {country}, Highest VEI: {fmax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11rdFk2qCu8u"
   },
   "source": [
    "Finally, we have ALMOST REACHED THE END!!\n",
    "Since there is still quite a bit of missing data, we want to make use of what is still available.\n",
    "\n",
    "A very powerful tool in Python's magnificent collection of libraries is its beautiful graphing tools.\n",
    "\n",
    "Check out libraries such as [Seaborn](https://seaborn.pydata.org/) or [Matplotlib](https://matplotlib.org/stable/index.html) to create meaningful visualizations! **Your final task in this section requires the use of these libraries**\n",
    "\n",
    "**2.1.3:** *(15 points here)*\n",
    "\n",
    "- Based on the **unique names of volcanoes**, **filter names that have more than 4 datapoints under their name.**\n",
    "- Each datapoint in the dataframe refers to a recorded instance of a volcanic eruption.\n",
    "- Make **separate line graphs for each volcano** and **plot their VEIs over time.**\n",
    "\n",
    "**Make sure to properly label all parts of the graph appropriately to receive credit üëÄ** (like title, axes, legend, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvasG8eViN1h"
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "for volcano, vol_df in new_df.groupby(\"name\").filter(lambda x: len(x) > 4).groupby(\"name\"):\n",
    "    # display(vol_df)\n",
    "    seaborn.lineplot(data=vol_df, x=\"date\", y=\"vei\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gozs5JwKO7ar"
   },
   "source": [
    "### **Part 3: Fiery Jobs üöí (15 POINTS TOTAL)**\n",
    "\n",
    "Proficiency in SQL is also super important. SQL databases are essentially relational databases in which there are vast amounts of tabular data. which can often be used to connect with related tablular data. [This](https://www.w3schools.com/sql/) is a pretty good intro into learning more about SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfrjZ3wrO-R2"
   },
   "source": [
    "Check out this [tutorial](https://mode.com/sql-tutorial/introduction-to-sql/) for some clarifications on SQL.\n",
    "\n",
    "Now! We'll be using **`sqlite`** to access a database.\n",
    "* **!!!IMPORTANT!!! PLEASE Start by downloading the sql lite file and putting it in the same directory as this [KAGGLE DATASET URL](https://www.kaggle.com/datasets/kaggle/sf-salaries) (hit the 'download' button in the upper right). You need an account in order to download the dataset.**\n",
    "* Check out the description of the data so you know the table / column names.\n",
    "\n",
    "The following code will use `sqlite3` to create a database connection. `sqlite3` is the library in Python that assists in navigating through SQL databases.\n",
    "\n",
    "**Note:** If you are working on this assignment via Google Colab, sometimes the runtime resets and it will throw errors.\n",
    "\n",
    "***Instead of running through the entire notebook, run the notebook from the following code block and onwards:***\n",
    "- Click anywhere on the next code block.\n",
    "- Go up to where it says **'Runtime'** in the toolbar (right under the title of the notebook and **in between 'Insert' and 'Tools'**)\n",
    "- Hover over it and **click on the option** that says **'Run cell and below'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooUWEVfYPAkD"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd # Pandas was already imported from the previous sections\n",
    "\n",
    "conn = sqlite3.connect(\"database.sqlite\")\n",
    "crsr = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKtpHcoGKxWE"
   },
   "source": [
    "## **!!!IMPORTANT!!! PLEASE Start by downloading the sql lite file and putting it in the same directory as this [KAGGLE DATASET URL](https://www.kaggle.com/datasets/kaggle/sf-salaries) (hit the 'download' button in the upper right). You need an account in order to download the dataset.**\n",
    "\n",
    "##### If you cannot see anything when executing this code, that means you did not download the Kaggle dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnJiNMw7o9d8"
   },
   "outputs": [],
   "source": [
    "# This code will let you check out the different tables within the database.\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = crsr.execute(query).fetchall()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KutbMSr2pV-O"
   },
   "source": [
    "#### **Remember that each problem should be solved with a single SQL query.**\n",
    "**Note: All outputs must be shown**\n",
    "- Only include whatever fields are mentioned throughout each question, nothing more and nothing less.\n",
    "- Follow each instruction clearly\n",
    "\n",
    "#### **3.1.1: 2 Points**\n",
    "From the **Salaries** table, get the **average base pay** for firefighters (all job titles consisting of the word \"firefighter\" **(not case-sensitive)**) between the **years 2010 to 2015**.\n",
    "\n",
    "*Remember that firefighters that also occupy other professions are still considered firefighters.*\n",
    "\n",
    "*Hint: Look into [this](https://www.w3schools.com/mysql/mysql_wildcards.asp) üëÄ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wm6NlYqp3dI"
   },
   "outputs": [],
   "source": [
    "query = \"SELECT AVG(BasePay) FROM Salaries WHERE JobTitle LIKE '%firefighter%' AND Year BETWEEN 2010 AND 2015\"\n",
    "\n",
    "\n",
    "# KEEP THIS. It will display the whole dataframe.\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwWzAyYbq9yE"
   },
   "source": [
    "#### **3.1.2: 2 Points**\n",
    "From the **Salaries** table, get all the firefighters (all job titles consisting of the word \"firefighter\" **(not case-sensitive)**) in the **year 2011** making under **$40,000 as a base pay.** **Sort** them in **descending** order by their pay.\n",
    "\n",
    "*Remember that firefighters that also occupy other professions are still considered firefighters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ_hK1tpq83U"
   },
   "outputs": [],
   "source": [
    "query = \"SELECT JobTitle, BasePay FROM Salaries WHERE JobTitle LIKE '%firefighter%' AND BasePay < 40000 AND Year = 2011 ORDER BY BasePay DESC\"\n",
    "# KEEP THIS. It will display the whole dataframe.\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNRnMM7Cq9gG"
   },
   "source": [
    "#### **3.1.3: 4 Points**\n",
    "From the **Salaries** table, first get the **averages** of **base pay**, **benefits**, and **overtime pay** for firefighters (all job titles consisting of the word \"firefighter\" **(not case-sensitive)**).\n",
    "\n",
    "- Then, make a **column with the sum** of these **three averages**\n",
    "- Finally, **exclude** job titles containing \"FIREFIGHTER\" **(case-sensitive)**\n",
    "\n",
    "*Remember that firefighters that also occupy other professions are still considered firefighters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVOCgk2Jq9LB"
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT AVG(BasePay), AVG(Benefits), AVG(OvertimePay), (AVG(BasePay) + AVG(Benefits) + AVG(OvertimePay))\n",
    "FROM Salaries\n",
    "WHERE JobTitle LIKE '%firefighter%' AND NOT JobTitle LIKE 'FIREFIGHTER'\n",
    "\n",
    "'''\n",
    "\n",
    "# KEEP THIS. It will display the whole dataframe.\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cp88tfxq-Ui"
   },
   "source": [
    "#### **3.1.4: 7 Points**\n",
    "\n",
    "Finally, we'll make our own table in our database.\n",
    "\n",
    "* Separate the **Salaries table** by **years (2010-2015)**, and add it back to the database.\n",
    "- Using a loop might be helpful.\n",
    "\n",
    "* You may use basic python to complete the task. However, using querying on SQL is **mandatory**.\n",
    "* Feel free to **use multiple lines of code for this problem only.**\n",
    "\n",
    "*Hint: Check [this](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) out*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjQnj4Uqq9C7"
   },
   "outputs": [],
   "source": [
    "# REMOVE THIS CONTENT AND ANSWER IN YOUR OWN WAY\n",
    "query = ''' \n",
    "SELECT * FROM Salaries\n",
    "WHERE Year AND Year >= 2010 AND Year <= 2015\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "for year, dataf in df.groupby(by=[\"Year\"]):\n",
    "    # print(year[0])\n",
    "    dataf.to_sql(name=str(year[0]), con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYcadk9g4jE1"
   },
   "outputs": [],
   "source": [
    "# Run this code to check if you successfully added your table.\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oW-M2Epk28g4"
   },
   "source": [
    "![volcano](https://as1.ftcdn.net/v2/jpg/06/34/76/64/1000_F_634766457_0fZbpYj6aBLlldO1jADUPpKTRLnNmngs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEbnutIFRCZN"
   },
   "source": [
    "### **Part 4: Pandas 'Group By'(40 points)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ9gKoZSRrJJ"
   },
   "source": [
    "This flowchart is taken from our lecture class presentation and illustrates the process of transforming data using the Pandas GroupBy operation. First, the data is input, followed by applying the GroupBy function to one or more columns of the DataFrame. Once the data is grouped, an aggregation function (such as sum(), mean(), or count()) is applied to compute summary statistics for each group\n",
    "\n",
    "\n",
    "\n",
    "![bonus](https://drive.google.com/uc?export=view&id=1OeO3cVJSgmk6QxeBUp5ojJQUGVAPB-Jx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxXmXcgSVXKj"
   },
   "source": [
    "Your task is to translate the workflow shown in the flowchart into Pandas queries that perform these operations step by step.\n",
    "\n",
    "**Notes:** Your task is to translate the workflow shown in the flowchart into Pandas queries. Ensure that the **exact input and exact output** from the flowchart are replicated using Pandas queries, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nfJ-BIaWGdM"
   },
   "source": [
    "**4.1 (Point 10 )** Create a sample dataset that includes columns for account, order, and ext price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXHDhmixVaAA"
   },
   "outputs": [],
   "source": [
    "# Create the Sample dataset from above flowchart\n",
    "input_data = {\n",
    "    'account': ['383080', '383080', '383080', '412290', '412290','412290','412290','412290','218895','218895','218895','218895'],\n",
    "    'order': ['10001', '10001', '10001', '10005', '10005','10005','10005','10005','10006','10006','10006','10006'],\n",
    "    'ext_price': [235.83, 232.32, 107.97, 2679.36, 286.02, 832.95, 3472.04, 915.12, 3061.12, 518.65, 216.9, -72.18]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df2 = pd.DataFrame(input_data)\n",
    "\n",
    "# Display Dataframe (DONT REMOVE THE CODE)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak4Cg4JFYHgt"
   },
   "source": [
    "**4.2 (Point 10+10 =20 )** Group by order and show the intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQSg8--oXxiq"
   },
   "outputs": [],
   "source": [
    "for ind, dataf in df2.groupby([\"order\"]):\n",
    "    display(dataf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEPVNJ2OYzCr"
   },
   "source": [
    "**4.3 (Point 10 )** Apply the Sum Aggregation for Each Group\n",
    "\n",
    "Now we'll apply the sum aggregation to get the total ext price for each order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WENrYhWJY4SD"
   },
   "outputs": [],
   "source": [
    "# Repeat group by 'order' again and then apply aggregation (sum of 'ext_price' for each 'order')\n",
    "\n",
    "df3 = df2.groupby([\"order\"])[\"ext_price\"].sum()\n",
    "\n",
    "# Show the aggregated result after re-grouping  (DONT REMOVE THE CODE)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdPWDJDraaWz"
   },
   "source": [
    "**4.4 (Point 10)**  Combine the Results into One Final Table\n",
    "Finally, we will **reset the index** and create a combined table that shows order and the sum of the ext price for each group:\n",
    "\n",
    "\n",
    "**Notes**: In pandas, 'reset_index()' is a method used to reset the index of a DataFrame to its default integer-based index. By default, when you perform certain operations like groupby(), the resulting DataFrame may have a new index (e.g., the grouped column). The reset_index() method allows you to convert the current index back to a default sequential integer index and optionally, move the current index values into a regular column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OZBE_Mhaa02"
   },
   "outputs": [],
   "source": [
    "# Reset index to combine result into a single DataFrame\n",
    "df3.reset_index()\n",
    "# Rename the columns for clarity\n",
    "\n",
    "# Show the final result  (DONT REMOVE THE CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrhhMBQGZAgz"
   },
   "source": [
    "# Bonus(1 point): Assignment Submission with Jupyter Notebook file(ipynb) and PDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6KHc02JZiMb"
   },
   "source": [
    "In addition to the instruction of the special bonus question included exclusively in Assignment 1, we would like to highlight the importance of developing good practices for assignment submission in data science projects.\n",
    "\n",
    "Think of yourself as a reviewer of your peers‚Äô work: what would you pay attention to if you had to grade their assignments? Clearly, the raw dataset itself is not the main point‚Äîwhy would someone need to see every line of data you downloaded? Data science is about analysis and insight, not just data collection. What truly matters is how you analyze the data and how you present your results.\n",
    "\n",
    "For a clean and effective submission, please avoid printing entire datasets unless they are directly relevant to the presentation of your output or if you are asked to do so. Instead, focus on:\n",
    "\n",
    "*   Showing your algorithm with clear, concise code\n",
    "*   Displaying the key results you want us to evaluate (or the outputs you relied on while working)\n",
    "*   Adding a short explanation when asked to interpret your findings\n",
    "\n",
    "For this bonus question, please submit both your .ipynb file and the PDF version. To create a PDF: go to File (top left corner in Jupyter Notebook) ‚Üí Print ‚Üí Destination ‚Üí Save as PDF.\n",
    "\n",
    "Are you noticing an overwhelming amount of unnecessary printed results in your PDF? This is exactly why we emphasize the importance of cleaning up your work before submission. Removing trivial data printing makes your notebook look much more professional and easier to review‚Äîjust like proofreading an essay for good grammar.\n",
    "\n",
    "Your Homework 1 submission should be lightweight and focused, since this is a starter project designed to help you ease into the semester. :)\n",
    "\n",
    "This practice helps us grade more efficiently and ensures fairness, since we can always reference your original notebook if regrading is needed."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
